{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "common_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in {dialect}. Your job is to read and understand the following [Database Schema] description, along with any [Reference Information], and then use your knowledge of {dialect} to generate an SQL statement that answers the [User Question]. Pay attention to the [Database Schema], only use tables and columns that are in the [Database Schema]. Avoid using any other tables or columns that are not in the [Database Schema].\n",
    "\n",
    "[User Question]\n",
    "{user_question}\n",
    "\n",
    "[Database Schema]\n",
    "{schema}\n",
    "\n",
    "[Reference Information]\n",
    "{example_rows}\n",
    "\n",
    "ONLY OUTPUT THE SQL STATEMENT, NO OTHER TEXT.\n",
    "\"\"\"\n",
    ")\n",
    "xiyan_en_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in {dialect}. Your job is to read and understand the following 【Schema】 description, along with any 【Evidence】, and then use your knowledge of {dialect} to generate an SQL statement that answers the 【Question】.\n",
    "\n",
    "【Question】\n",
    "{user_question}\n",
    "\n",
    "【Schema】\n",
    "{schema}\n",
    "\n",
    "【Evidence】\n",
    "{example_rows}\n",
    "\n",
    "【Question】\n",
    "{user_question}\n",
    "```sql\n",
    "\"\"\"\n",
    ")\n",
    "xiyan_cn_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"你是一名{dialect}专家，现在需要阅读并理解下面的【数据库schema】描述，以及可能用到的【参考信息】，并运用{dialect}知识生成sql语句回答【用户问题】。\n",
    "【用户问题】\n",
    "{user_question}\n",
    "\n",
    "【数据库schema】\n",
    "{schema}\n",
    "\n",
    "【参考信息】\n",
    "{example_rows}\n",
    "\n",
    "【用户问题】\n",
    "{user_question}\n",
    "\n",
    "```sql\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llama_model_base_config = {\n",
    "    \"model_name\": \"llama3.2:3b\",\n",
    "    \"model_config_init\": {\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    "    \"prompt_template\": common_prompt_template,\n",
    "    \"inference_type\": \"ollama\",\n",
    "    \"use_mschema\": False,\n",
    "}\n",
    "xiyan7b16_model_base_config = {\n",
    "    \"model_name\": \"hf.co/mradermacher/XiYanSQL-QwenCoder-7B-2504-GGUF:F16\",\n",
    "    \"model_config_init\": {\"temperature\": 0.1, \"top_p\": 0.8},\n",
    "    \"prompt_template\": xiyan_en_prompt_template,\n",
    "    \"inference_type\": \"ollama\",\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"llama32_3b_base\": {\n",
    "        **llama_model_base_config,\n",
    "        \"use_example_rows\": False,\n",
    "        \"retry_times\": 0,\n",
    "    },\n",
    "    \"llama32_3b_w_ex\": {\n",
    "        **llama_model_base_config,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 0,\n",
    "    },\n",
    "    \"llama32_3b_w_re\": {\n",
    "        **llama_model_base_config,\n",
    "        \"use_example_rows\": False,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    \"llama32_3b_all\": {\n",
    "        **llama_model_base_config,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    \"llama31_8b_all\": {\n",
    "        **llama_model_base_config,\n",
    "        \"model_name\": \"llama3.1:8b\",\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    # \"llama31_8b_all\": {\n",
    "    #     \"prompt_template\": common_prompt_template,\n",
    "    #     \"use_mschema\": False,\n",
    "    #     \"model_name\": \"meta-llama/Llama-3.1-8B\",\n",
    "    #     \"inference_type\": \"vllm\",\n",
    "    #     \"use_example_rows\": True,\n",
    "    #     \"retry_times\": 10,\n",
    "    #     \"model_config_init\": {\n",
    "    #         \"temperature\": 0.1,\n",
    "    #         \"vllm_kwargs\": {\n",
    "    #             \"hf_token\": os.environ[\"HF_TOKEN\"],\n",
    "    #             \"max_model_len\": 20000,\n",
    "    #         },\n",
    "    #     },\n",
    "    # },\n",
    "    \"xiyansql_7b_16_base\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": False,\n",
    "        \"retry_times\": 0,\n",
    "    },\n",
    "    \"xiyansql_7b_16_w_ex\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 0,\n",
    "    },\n",
    "    \"xiyansql_7b_16_w_re\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": False,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    \"xiyansql_7b_16_all\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    \"xiyansql_7b_8_all\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"model_name\": \"hf.co/mradermacher/XiYanSQL-QwenCoder-7B-2504-GGUF:Q8_0\",\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.llms import VLLM\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "class ModelCreater:\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.model_name = None\n",
    "        self.infer_type = None\n",
    "\n",
    "    def create_model(self, model_config: dict):\n",
    "        if (\n",
    "            self.llm\n",
    "            and self.model_name == model_config[\"model_name\"]\n",
    "            and self.infer_type == model_config[\"inference_type\"]\n",
    "        ):\n",
    "            return self.llm\n",
    "        else:\n",
    "            # shutdown created vllm\n",
    "            if self.infer_type == \"vllm\":\n",
    "                pass\n",
    "\n",
    "            if model_config[\"inference_type\"] == \"ollama\":\n",
    "                self.llm = ChatOllama(\n",
    "                    model=model_config[\"model_name\"],\n",
    "                    **model_config[\"model_config_init\"],\n",
    "                )\n",
    "            elif model_config[\"inference_type\"] == \"vllm\":\n",
    "                self.llm = VLLM(\n",
    "                    model=model_config[\"model_name\"],\n",
    "                    trust_remote_code=True,\n",
    "                    **model_config[\"model_config_init\"],\n",
    "                )\n",
    "            elif model_config[\"inference_type\"] == \"vllm-openai\":\n",
    "                self.llm = ChatOpenAI(\n",
    "                    model=model_config[\"model_name\"],\n",
    "                    base_url=\"http://127.0.0.1:8000/\",\n",
    "                    api_key=\"EMPTY\",\n",
    "                    **model_config[\"model_config_init\"],\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Inference type {model_config['inference_type']} not supported\"\n",
    "                )\n",
    "        self.model_name = model_config[\"model_name\"]\n",
    "        self.infer_type = model_config[\"inference_type\"]\n",
    "        return self.llm\n",
    "\n",
    "\n",
    "model_creater = ModelCreater()\n",
    "retry_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Your query is invalid. Carefully read the table schemas and the user question, and then regenerate a new query.\n",
    "    \n",
    "User question:\n",
    "{user_question}\n",
    "\n",
    "Table schemas:\n",
    "{schema}\n",
    "\n",
    "Here is your generated query:\n",
    "{invalid_query}\n",
    "\n",
    "And here is the error:\n",
    "{error_message}\n",
    "\n",
    "ONLY OUTPUT THE SQL STATEMENT, NO OTHER TEXT.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def check_sql_valid(sql: str, db_path: str):\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"EXPLAIN \" + sql))\n",
    "    except Exception as e:\n",
    "        return False, repr(e)\n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "def gen_sql(llm, prompt: PromptValue):\n",
    "    message = llm.invoke(prompt)\n",
    "    if not isinstance(message, str):\n",
    "        message = message.content\n",
    "    message = (\n",
    "        message.strip()\n",
    "        .replace(\"```sql\", \"\")\n",
    "        .replace(\"```\", \"\")\n",
    "        .replace(\"\\n\", \"\")\n",
    "        .replace(\"\\t\", \"\")\n",
    "    )\n",
    "    return message\n",
    "\n",
    "\n",
    "def benchmark_model(\n",
    "    model: str,\n",
    "    dialect: str,\n",
    "    input_file_path: str,\n",
    "    output_file_path: str,\n",
    "    db_root_dir: str,\n",
    "):\n",
    "    config = model_config[model]\n",
    "    llm = model_creater.create_model(config)\n",
    "    prompt_template = config[\"prompt_template\"]\n",
    "\n",
    "    def generate_sql(\n",
    "        user_question: str,\n",
    "        schema: str,\n",
    "        dialect: str,\n",
    "        db_id: str,\n",
    "        example_rows: list[str] | None = None,\n",
    "    ):\n",
    "        prompt_msg = prompt_template.invoke(\n",
    "            {\n",
    "                \"user_question\": user_question,\n",
    "                \"schema\": schema,\n",
    "                \"dialect\": dialect,\n",
    "                \"example_rows\": example_rows if example_rows else \"None\",\n",
    "            }\n",
    "        )\n",
    "        message = gen_sql(llm, prompt_msg)\n",
    "\n",
    "        if config[\"retry_times\"] > 0:\n",
    "            db_path = f\"{db_root_dir}/{db_id}/{db_id}.sqlite\"\n",
    "            is_valid, err_msg = check_sql_valid(message, db_path)\n",
    "\n",
    "            i = 0\n",
    "            while not is_valid and i < config[\"retry_times\"]:\n",
    "                retry_prompt_msg = retry_prompt_template.invoke(\n",
    "                    {\n",
    "                        \"user_question\": user_question,\n",
    "                        \"schema\": schema,\n",
    "                        \"invalid_query\": message,\n",
    "                        \"error_message\": err_msg,\n",
    "                    }\n",
    "                )\n",
    "                # print(\n",
    "                #     f\"({i + 1}/{config['retry_times']}) Retry prompt: {retry_prompt_msg}\"\n",
    "                # )\n",
    "                message = gen_sql(llm, retry_prompt_msg)\n",
    "                is_valid, err_msg = check_sql_valid(message, db_path)\n",
    "                i += 1\n",
    "        return message\n",
    "\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    inference_df = df.assign(\n",
    "        answer=lambda df_: df_.progress_apply(\n",
    "            lambda row: generate_sql(\n",
    "                user_question=row.question,\n",
    "                schema=row.schemas if not config[\"use_mschema\"] else row.mschemas,\n",
    "                dialect=dialect,\n",
    "                db_id=row.db_id,\n",
    "                example_rows=row.example_rows if config[\"use_example_rows\"] else None,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "    with open(output_file_path, \"w+\") as f:\n",
    "        answer_list = inference_df[\"answer\"].tolist()\n",
    "        f.write(\"\\n\".join(answer_list))\n",
    "\n",
    "    return inference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "    # (\"llama32_3b_base\", \"dev\"),\n",
    "    # (\"llama32_3b_w_ex\", \"dev\"),\n",
    "    # (\"llama32_3b_w_re\", \"dev\"),\n",
    "    # (\"llama32_3b_all\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_base\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_w_ex\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_w_re\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_all\", \"dev\"),\n",
    "    # (\"llama32_3b_all\", \"test\"),\n",
    "    (\"llama31_8b_all\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_all\", \"test\"),\n",
    "    # (\"xiyansql_7b_8_all\", \"test\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking llama31_8b_all, dev dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62d7cd64cbb401980eab99818e302a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_name, dataset in test_list:\n",
    "    dialect = \"sqlite\"\n",
    "    input_file_path = f\"prepare_data/{dataset}_input.csv\"\n",
    "    output_file_path = f\"inference_data/{model_name}_{dataset}_inf.txt\"\n",
    "    db_root_dir = (\n",
    "        \"spider_data/database\" if dataset == \"dev\" else \"spider_data/test_database\"\n",
    "    )\n",
    "    print(f\"Benchmarking {model_name}, {dataset} dataset\")\n",
    "    inference_df = benchmark_model(\n",
    "        model_name, dialect, input_file_path, output_file_path, db_root_dir\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
