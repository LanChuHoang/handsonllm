{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "common_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in {dialect}. Your job is to read and understand the following [Database Schema] description, along with any [Reference Information], and then use your knowledge of {dialect} to generate an SQL statement that answers the [User Question]. Pay attention to the [Database Schema], only use tables and columns that are in the [Database Schema]. Avoid using any other tables or columns that are not in the [Database Schema].\n",
    "\n",
    "[User Question]\n",
    "{user_question}\n",
    "\n",
    "[Database Schema]\n",
    "{schema}\n",
    "\n",
    "[Reference Information]\n",
    "None\n",
    "\n",
    "ONLY OUTPUT THE SQL STATEMENT, NO OTHER TEXT.\n",
    "\"\"\"\n",
    ")\n",
    "xiyan_en_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in {dialect}. Your job is to read and understand the following [Database Schema] description, along with any [Reference Information], and then use your knowledge of {dialect} to generate an SQL statement that answers the [User Question].\n",
    "\n",
    "[User Question]\n",
    "{user_question}\n",
    "\n",
    "[Database Schema]\n",
    "{schema}\n",
    "\n",
    "[Reference Information]\n",
    "None\n",
    "\n",
    "[User Question]\n",
    "{user_question}\n",
    "```sql\n",
    "\"\"\"\n",
    ")\n",
    "xiyan_cn_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"你是一名{dialect}专家，现在需要阅读并理解下面的【数据库schema】描述，以及可能用到的【参考信息】，并运用{dialect}知识生成sql语句回答【用户问题】。\n",
    "【用户问题】\n",
    "{user_question}\n",
    "\n",
    "【数据库schema】\n",
    "{schema}\n",
    "\n",
    "【参考信息】\n",
    "None\n",
    "\n",
    "【用户问题】\n",
    "{user_question}\n",
    "\n",
    "```sql\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"llama32_3b\": {\n",
    "        \"model_name\": \"llama3.2:3b\",\n",
    "        \"model_config_init\": {\n",
    "            \"temperature\": 0.0,\n",
    "        },\n",
    "        \"prompt_template\": common_prompt_template,\n",
    "    },\n",
    "    \"sqlllama_7b_16\": {\n",
    "        \"model_name\": \"hf.co/mradermacher/SQL-Llama-v0.5-GGUF:F16\",\n",
    "        \"model_config_init\": {\n",
    "            \"temperature\": 0.0,\n",
    "        },\n",
    "        \"prompt_template\": common_prompt_template,\n",
    "    },\n",
    "    \"xiyansql_7b_8_en_prompt\": {\n",
    "        \"model_name\": \"hf.co/mradermacher/XiYanSQL-QwenCoder-7B-2504-GGUF:Q8_0\",\n",
    "        \"model_config_init\": {\n",
    "            \"temperature\": 0.0,\n",
    "        },\n",
    "        \"prompt_template\": xiyan_en_prompt_template,\n",
    "    },\n",
    "    \"xiyansql_7b_8_cn_prompt\": {\n",
    "        \"model_name\": \"hf.co/mradermacher/XiYanSQL-QwenCoder-7B-2504-GGUF:Q8_0\",\n",
    "        \"model_config_init\": {\n",
    "            \"temperature\": 0.0,\n",
    "        },\n",
    "        \"prompt_template\": xiyan_cn_prompt_template,\n",
    "    },\n",
    "    \"xiyansql_7b_16_en_prompt\": {\n",
    "        \"model_name\": \"hf.co/mradermacher/XiYanSQL-QwenCoder-7B-2504-GGUF:F16\",\n",
    "        \"model_config_init\": {\"temperature\": 0.1, \"top_p\": 0.8},\n",
    "        \"prompt_template\": xiyan_en_prompt_template,\n",
    "    },\n",
    "    \"xiyansql_7b_16_cn_prompt\": {\n",
    "        \"model_name\": \"hf.co/mradermacher/XiYanSQL-QwenCoder-7B-2504-GGUF:F16\",\n",
    "        \"model_config_init\": {\"temperature\": 0.1, \"top_p\": 0.8},\n",
    "        \"prompt_template\": xiyan_cn_prompt_template,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def benchmark_model(\n",
    "    model: str, input_file_path: str, output_file_path: str, dialect: str\n",
    "):\n",
    "    config = model_config[model]\n",
    "    llm = ChatOllama(model=config[\"model_name\"], **config[\"model_config_init\"])\n",
    "    prompt_template = config[\"prompt_template\"]\n",
    "\n",
    "    def generate_sql(user_question: str, schema_list: list[str], dialect: str):\n",
    "        message = prompt_template.invoke(\n",
    "            {\n",
    "                \"user_question\": user_question,\n",
    "                \"schema\": \"\\n\".join(schema_list),\n",
    "                \"dialect\": dialect,\n",
    "            }\n",
    "        )\n",
    "        message = llm.invoke(message).content\n",
    "        # format the message to be a valid sql statement\n",
    "        message = (\n",
    "            message.strip()\n",
    "            .replace(\"```sql\", \"\")\n",
    "            .replace(\"```\", \"\")\n",
    "            .replace(\"\\n\", \"\")\n",
    "            .replace(\"\\t\", \"\")\n",
    "        )\n",
    "        # print(f\"Generated answer for question: {user_question[:20]}...\")\n",
    "        return message\n",
    "\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    inference_df = df.assign(\n",
    "        answer=lambda df_: df_.progress_apply(\n",
    "            lambda row: generate_sql(row.question, row.schemas, dialect), axis=1\n",
    "        )\n",
    "    )\n",
    "    with open(output_file_path, \"w+\") as f:\n",
    "        answer_list = inference_df[\"answer\"].tolist()\n",
    "        f.write(\"\\n\".join(answer_list))\n",
    "\n",
    "    return inference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "    # \"llama32_3b\",\n",
    "    \"sqlllama_7b_16\",\n",
    "    \"xiyansql_7b_8_en_prompt\",\n",
    "    \"xiyansql_7b_8_cn_prompt\",\n",
    "    # \"xiyansql_7b_16_en_prompt\",\n",
    "    # \"xiyansql_7b_16_cn_prompt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking sqlllama_7b_16, dev dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb8d44bec8f48b2b953045c6f2212fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking xiyansql_7b_8_en_prompt, dev dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5009f465770d411aa7dd3dffbf71924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking xiyansql_7b_8_cn_prompt, dev dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0df60cd2e014bc6bbdf9dc0d44d965d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking sqlllama_7b_16, test dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6425a92de4a4667a93d606f5b9c48dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking xiyansql_7b_8_en_prompt, test dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9dd2c8dd8b42b48376509265ca31a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset in [\"dev\", \"test\"]:\n",
    "    for model_name in test_list:\n",
    "        dialect = \"sqlite\"\n",
    "        input_file_path = f\"prepare_data/{dataset}_input.csv\"\n",
    "        output_file_path = f\"inference_data/{model_name}_{dataset}_inf.txt\"\n",
    "        print(f\"Benchmarking {model_name}, {dataset} dataset\")\n",
    "        inference_df = benchmark_model(\n",
    "            model_name, input_file_path, output_file_path, dialect\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
