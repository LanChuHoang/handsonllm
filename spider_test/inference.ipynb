{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "common_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in {dialect}. Your job is to read and understand the following [Database Schema] description, along with any [Reference Information], and then use your knowledge of {dialect} to generate an SQL statement that answers the [User Question]. Pay attention to the [Database Schema], only use tables and columns that are in the [Database Schema]. Avoid using any other tables or columns that are not in the [Database Schema].\n",
    "\n",
    "[User Question]\n",
    "{user_question}\n",
    "\n",
    "[Database Schema]\n",
    "{schema}\n",
    "\n",
    "[Reference Information]\n",
    "{example_rows}\n",
    "\n",
    "ONLY OUTPUT THE SQL STATEMENT, NO OTHER TEXT.\n",
    "\"\"\"\n",
    ")\n",
    "xiyan_en_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in {dialect}. Your job is to read and understand the following 【Schema】 description, along with any 【Evidence】, and then use your knowledge of {dialect} to generate an SQL statement that answers the 【Question】.\n",
    "\n",
    "【Question】\n",
    "{user_question}\n",
    "\n",
    "【Schema】\n",
    "{schema}\n",
    "\n",
    "【Evidence】\n",
    "{example_rows}\n",
    "\n",
    "【Question】\n",
    "{user_question}\n",
    "```sql\n",
    "\"\"\"\n",
    ")\n",
    "xiyan_cn_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"你是一名{dialect}专家，现在需要阅读并理解下面的【数据库schema】描述，以及可能用到的【参考信息】，并运用{dialect}知识生成sql语句回答【用户问题】。\n",
    "【用户问题】\n",
    "{user_question}\n",
    "\n",
    "【数据库schema】\n",
    "{schema}\n",
    "\n",
    "【参考信息】\n",
    "{example_rows}\n",
    "\n",
    "【用户问题】\n",
    "{user_question}\n",
    "\n",
    "```sql\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llama_model_base_config = {\n",
    "    \"model_name\": \"llama3.2:3b\",\n",
    "    \"model_config_init\": {\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    "    \"prompt_template\": common_prompt_template,\n",
    "    \"inference_type\": \"ollama\",\n",
    "    \"use_mschema\": False,\n",
    "}\n",
    "xiyan7b16_model_base_config = {\n",
    "    \"model_name\": \"hf.co/mradermacher/XiYanSQL-QwenCoder-7B-2504-GGUF:F16\",\n",
    "    \"model_config_init\": {\"temperature\": 0.1, \"top_p\": 0.8},\n",
    "    \"prompt_template\": xiyan_en_prompt_template,\n",
    "    \"inference_type\": \"ollama\",\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"llama32_3b_base\": {\n",
    "        **llama_model_base_config,\n",
    "        \"use_example_rows\": False,\n",
    "        \"retry_times\": 0,\n",
    "    },\n",
    "    \"llama32_3b_w_ex\": {\n",
    "        **llama_model_base_config,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 0,\n",
    "    },\n",
    "    \"llama32_3b_w_re\": {\n",
    "        **llama_model_base_config,\n",
    "        \"use_example_rows\": False,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    \"llama32_3b_all\": {\n",
    "        **llama_model_base_config,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    # \"llama31_8b_all\": {\n",
    "    #     **llama_model_base_config,\n",
    "    #     \"model_name\": \"hf.co/mradermacher/Meta-Llama-3.1-8B-Instruct-GGUF:F16\",\n",
    "    #     \"use_example_rows\": True,\n",
    "    #     \"retry_times\": 10,\n",
    "    # },\n",
    "    \"llama31_8b_all\": {\n",
    "        \"prompt_template\": common_prompt_template,\n",
    "        \"use_mschema\": False,\n",
    "        \"model_name\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"inference_type\": \"vllm\",\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 10,\n",
    "        \"model_config_init\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"vllm_kwargs\": {\n",
    "                \"hf_token\": os.environ[\"HF_TOKEN\"],\n",
    "                \"max_model_len\": 20000,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"xiyansql_7b_16_base\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": False,\n",
    "        \"retry_times\": 0,\n",
    "    },\n",
    "    \"xiyansql_7b_16_w_ex\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 0,\n",
    "    },\n",
    "    \"xiyansql_7b_16_w_re\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": False,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    \"xiyansql_7b_16_all\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "    \"xiyansql_7b_8_all\": {\n",
    "        **xiyan7b16_model_base_config,\n",
    "        \"model_name\": \"hf.co/mradermacher/XiYanSQL-QwenCoder-7B-2504-GGUF:Q8_0\",\n",
    "        \"use_mschema\": False,\n",
    "        \"use_example_rows\": True,\n",
    "        \"retry_times\": 10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.llms import VLLM\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "class ModelCreater:\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.model_name = None\n",
    "        self.infer_type = None\n",
    "\n",
    "    def create_model(self, model_config: dict):\n",
    "        if (\n",
    "            self.llm\n",
    "            and self.model_name == model_config[\"model_name\"]\n",
    "            and self.infer_type == model_config[\"inference_type\"]\n",
    "        ):\n",
    "            return self.llm\n",
    "        else:\n",
    "            # shutdown created vllm\n",
    "            if self.infer_type == \"vllm\":\n",
    "                pass\n",
    "\n",
    "            if model_config[\"inference_type\"] == \"ollama\":\n",
    "                self.llm = ChatOllama(\n",
    "                    model=model_config[\"model_name\"],\n",
    "                    **model_config[\"model_config_init\"],\n",
    "                )\n",
    "            elif model_config[\"inference_type\"] == \"vllm\":\n",
    "                self.llm = VLLM(\n",
    "                    model=model_config[\"model_name\"],\n",
    "                    trust_remote_code=True,\n",
    "                    **model_config[\"model_config_init\"],\n",
    "                )\n",
    "            elif model_config[\"inference_type\"] == \"vllm-openai\":\n",
    "                self.llm = ChatOpenAI(\n",
    "                    model=model_config[\"model_name\"],\n",
    "                    base_url=\"http://127.0.0.1:8000/\",\n",
    "                    api_key=\"EMPTY\",\n",
    "                    **model_config[\"model_config_init\"],\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Inference type {model_config['inference_type']} not supported\"\n",
    "                )\n",
    "        self.model_name = model_config[\"model_name\"]\n",
    "        self.infer_type = model_config[\"inference_type\"]\n",
    "        return self.llm\n",
    "\n",
    "\n",
    "model_creater = ModelCreater()\n",
    "retry_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Your query is invalid. Carefully read the table schemas and the user question, and then regenerate a new query.\n",
    "    \n",
    "User question:\n",
    "{user_question}\n",
    "\n",
    "Table schemas:\n",
    "{schema}\n",
    "\n",
    "Here is your generated query:\n",
    "{invalid_query}\n",
    "\n",
    "And here is the error:\n",
    "{error_message}\n",
    "\n",
    "ONLY OUTPUT THE SQL STATEMENT, NO OTHER TEXT.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def check_sql_valid(sql: str, db_path: str):\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"EXPLAIN \" + sql))\n",
    "    except Exception as e:\n",
    "        return False, repr(e)\n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "def gen_sql(llm, prompt: PromptValue):\n",
    "    message = llm.invoke(prompt)\n",
    "    if not isinstance(message, str):\n",
    "        message = message.content\n",
    "    message = (\n",
    "        message.strip()\n",
    "        .replace(\"```sql\", \"\")\n",
    "        .replace(\"```\", \"\")\n",
    "        .replace(\"\\n\", \"\")\n",
    "        .replace(\"\\t\", \"\")\n",
    "    )\n",
    "    return message\n",
    "\n",
    "\n",
    "def benchmark_model(\n",
    "    model: str,\n",
    "    dialect: str,\n",
    "    input_file_path: str,\n",
    "    output_file_path: str,\n",
    "    db_root_dir: str,\n",
    "):\n",
    "    config = model_config[model]\n",
    "    llm = model_creater.create_model(config)\n",
    "    prompt_template = config[\"prompt_template\"]\n",
    "\n",
    "    def generate_sql(\n",
    "        user_question: str,\n",
    "        schema: str,\n",
    "        dialect: str,\n",
    "        db_id: str,\n",
    "        example_rows: list[str] | None = None,\n",
    "    ):\n",
    "        prompt_msg = prompt_template.invoke(\n",
    "            {\n",
    "                \"user_question\": user_question,\n",
    "                \"schema\": schema,\n",
    "                \"dialect\": dialect,\n",
    "                \"example_rows\": example_rows if example_rows else \"None\",\n",
    "            }\n",
    "        )\n",
    "        message = gen_sql(llm, prompt_msg)\n",
    "\n",
    "        if config[\"retry_times\"] > 0:\n",
    "            db_path = f\"{db_root_dir}/{db_id}/{db_id}.sqlite\"\n",
    "            is_valid, err_msg = check_sql_valid(message, db_path)\n",
    "\n",
    "            i = 0\n",
    "            while not is_valid and i < config[\"retry_times\"]:\n",
    "                retry_prompt_msg = retry_prompt_template.invoke(\n",
    "                    {\n",
    "                        \"user_question\": user_question,\n",
    "                        \"schema\": schema,\n",
    "                        \"invalid_query\": message,\n",
    "                        \"error_message\": err_msg,\n",
    "                    }\n",
    "                )\n",
    "                # print(\n",
    "                #     f\"({i + 1}/{config['retry_times']}) Retry prompt: {retry_prompt_msg}\"\n",
    "                # )\n",
    "                message = gen_sql(llm, retry_prompt_msg)\n",
    "                is_valid, err_msg = check_sql_valid(message, db_path)\n",
    "                i += 1\n",
    "        return message\n",
    "\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    inference_df = df.assign(\n",
    "        answer=lambda df_: df_.progress_apply(\n",
    "            lambda row: generate_sql(\n",
    "                user_question=row.question,\n",
    "                schema=row.schemas if not config[\"use_mschema\"] else row.mschemas,\n",
    "                dialect=dialect,\n",
    "                db_id=row.db_id,\n",
    "                example_rows=row.example_rows if config[\"use_example_rows\"] else None,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "    with open(output_file_path, \"w+\") as f:\n",
    "        answer_list = inference_df[\"answer\"].tolist()\n",
    "        f.write(\"\\n\".join(answer_list))\n",
    "\n",
    "    return inference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "    # (\"llama32_3b_base\", \"dev\"),\n",
    "    # (\"llama32_3b_w_ex\", \"dev\"),\n",
    "    # (\"llama32_3b_w_re\", \"dev\"),\n",
    "    # (\"llama32_3b_all\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_base\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_w_ex\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_w_re\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_all\", \"dev\"),\n",
    "    # (\"llama32_3b_all\", \"test\"),\n",
    "    (\"llama31_8b_all\", \"dev\"),\n",
    "    # (\"xiyansql_7b_16_all\", \"test\"),\n",
    "    # (\"xiyansql_7b_8_all\", \"test\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking llama31_8b_all, dev dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7e2d20963310>\n",
      "FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b450a4c94e24f45a611aa3869a91605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EngineCore failed to start.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 506, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "    self._initialize_kv_caches(vllm_config)\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 141, in _initialize_kv_caches\n",
      "    available_gpu_memory = self.model_executor.determine_available_memory()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n",
      "    output = self.collective_rpc(\"determine_available_memory\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/utils.py\", line 2671, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 205, in determine_available_memory\n",
      "    self.model_runner.profile_run()\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2012, in profile_run\n",
      "    hidden_states = self._dummy_run(self.max_num_tokens)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1847, in _dummy_run\n",
      "    outputs = model(\n",
      "              ^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 581, in forward\n",
      "    model_output = self.model(input_ids, positions, intermediate_tensors,\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 239, in __call__\n",
      "    output = self.compiled_callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 663, in _fn\n",
      "    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1544, in _call_user_compiler\n",
      "    raise BackendCompilerFailed(\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1519, in _call_user_compiler\n",
      "    compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 150, in __call__\n",
      "    compiled_gm = compiler_fn(gm, example_inputs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 150, in __call__\n",
      "    compiled_gm = compiler_fn(gm, example_inputs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/__init__.py\", line 2392, in __call__\n",
      "    return self.compiler_fn(model_, inputs_, **self.kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/compilation/backends.py\", line 501, in __call__\n",
      "    PiecewiseCompileInterpreter(self.split_gm, submod_names_to_compile,\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/compilation/backends.py\", line 276, in run\n",
      "    return super().run(*fake_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/fx/interpreter.py\", line 151, in run\n",
      "    pbar = tqdm(\n",
      "           ^^^^^\n",
      "  File \"/tmp/ipykernel_7157/2618219602.py\", line 15, in <lambda>\n",
      "    tqdm.tqdm = lambda *args, **kwargs: args[0]  # no bars anywhere\n",
      "                                        ~~~~^^^\n",
      "torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7e2e1b7a9710>' raised:\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "\n",
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 519, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 506, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "    self._initialize_kv_caches(vllm_config)\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 141, in _initialize_kv_caches\n",
      "    available_gpu_memory = self.model_executor.determine_available_memory()\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n",
      "    output = self.collective_rpc(\"determine_available_memory\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/utils.py\", line 2671, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 205, in determine_available_memory\n",
      "    self.model_runner.profile_run()\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2012, in profile_run\n",
      "    hidden_states = self._dummy_run(self.max_num_tokens)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1847, in _dummy_run\n",
      "    outputs = model(\n",
      "              ^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 581, in forward\n",
      "    model_output = self.model(input_ids, positions, intermediate_tensors,\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 239, in __call__\n",
      "    output = self.compiled_callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 663, in _fn\n",
      "    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1544, in _call_user_compiler\n",
      "    raise BackendCompilerFailed(\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1519, in _call_user_compiler\n",
      "    compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 150, in __call__\n",
      "    compiled_gm = compiler_fn(gm, example_inputs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 150, in __call__\n",
      "    compiled_gm = compiler_fn(gm, example_inputs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/__init__.py\", line 2392, in __call__\n",
      "    return self.compiler_fn(model_, inputs_, **self.kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/compilation/backends.py\", line 501, in __call__\n",
      "    PiecewiseCompileInterpreter(self.split_gm, submod_names_to_compile,\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/compilation/backends.py\", line 276, in run\n",
      "    return super().run(*fake_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/llm/lib/python3.11/site-packages/torch/fx/interpreter.py\", line 151, in run\n",
      "    pbar = tqdm(\n",
      "           ^^^^^\n",
      "  File \"/tmp/ipykernel_7157/2618219602.py\", line 15, in <lambda>\n",
      "    tqdm.tqdm = lambda *args, **kwargs: args[0]  # no bars anywhere\n",
      "                                        ~~~~^^^\n",
      "torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7e2e1b7a9710>' raised:\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m db_root_dir = (\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mspider_data/database\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset == \u001b[33m\"\u001b[39m\u001b[33mdev\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mspider_data/test_database\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBenchmarking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dataset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m inference_df = \u001b[43mbenchmark_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_root_dir\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mbenchmark_model\u001b[39m\u001b[34m(model, dialect, input_file_path, output_file_path, db_root_dir)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbenchmark_model\u001b[39m(\n\u001b[32m    104\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    105\u001b[39m     dialect: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m     db_root_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    109\u001b[39m ):\n\u001b[32m    110\u001b[39m     config = model_config[model]\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     llm = \u001b[43mmodel_creater\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     prompt_template = config[\u001b[33m\"\u001b[39m\u001b[33mprompt_template\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_sql\u001b[39m(\n\u001b[32m    115\u001b[39m         question_number: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    116\u001b[39m         user_question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m         example_rows: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    121\u001b[39m     ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mModelCreater.create_model\u001b[39m\u001b[34m(self, model_config)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm = ChatOllama(\n\u001b[32m     33\u001b[39m         model=model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     34\u001b[39m         **model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_config_init\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_config[\u001b[33m\"\u001b[39m\u001b[33minference_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mvllm\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm = \u001b[43mVLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_config_init\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_config[\u001b[33m\"\u001b[39m\u001b[33minference_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mvllm-openai\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm = ChatOpenAI(\n\u001b[32m     44\u001b[39m         model=model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     45\u001b[39m         base_url=\u001b[33m\"\u001b[39m\u001b[33mhttp://127.0.0.1:8000/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m         api_key=\u001b[33m\"\u001b[39m\u001b[33mEMPTY\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m         **model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_config_init\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     48\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/pydantic/_internal/_decorators_v1.py:148\u001b[39m, in \u001b[36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[39m\u001b[34m(values, _)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/utils/pydantic.py:168\u001b[39m, in \u001b[36mpre_init.<locals>.wrapper\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m    165\u001b[39m             values[name] = field_info.default\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_community/llms/vllm.py:89\u001b[39m, in \u001b[36mVLLM.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import vllm python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install vllm`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m values[\u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mVLLModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensor_parallel_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrust_remote_code\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdownload_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvllm_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/entrypoints/llm.py:243\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m engine_args = EngineArgs(\n\u001b[32m    214\u001b[39m     model=model,\n\u001b[32m    215\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     **kwargs,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/engine/llm_engine.py:501\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    499\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:124\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    123\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:101\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     98\u001b[39m                                         log_stats=\u001b[38;5;28mself\u001b[39m.log_stats)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:75\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     72\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:558\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    557\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:422\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m.resources.output_socket = make_zmq_socket(\n\u001b[32m    419\u001b[39m     \u001b[38;5;28mself\u001b[39m.ctx, output_address, zmq.PULL)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client_addresses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_engines_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mlocal_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m                              \u001b[49m\u001b[43moutput_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m     coordinator = \u001b[38;5;28mself\u001b[39m.resources.coordinator\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coordinator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:491\u001b[39m, in \u001b[36mMPClient._init_engines_direct\u001b[39m\u001b[34m(self, vllm_config, local_only, local_start_index, input_address, output_address, executor_class, log_stats)\u001b[39m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28mself\u001b[39m.resources.engine_manager = CoreEngineProcManager(\n\u001b[32m    480\u001b[39m         EngineCoreProc.run_engine_core,\n\u001b[32m    481\u001b[39m         vllm_config=vllm_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m         start_index=start_index,\n\u001b[32m    488\u001b[39m         local_start_index=local_start_index)\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                              \u001b[49m\u001b[43moutput_address\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:511\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self, handshake_socket, input_address, output_address)\u001b[39m\n\u001b[32m    506\u001b[39m proc_manager = \u001b[38;5;28mself\u001b[39m.resources.engine_manager\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc_manager, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), CoreEngineProcManager)), (\n\u001b[32m    508\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m_wait_for_engine_startup should only be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    509\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalled with CoreEngineProcManager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_engines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproc_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/utils.py:494\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    493\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    499\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "for model_name, dataset in test_list:\n",
    "    dialect = \"sqlite\"\n",
    "    input_file_path = f\"prepare_data/{dataset}_input.csv\"\n",
    "    output_file_path = f\"inference_data/{model_name}_{dataset}_inf.txt\"\n",
    "    db_root_dir = (\n",
    "        \"spider_data/database\" if dataset == \"dev\" else \"spider_data/test_database\"\n",
    "    )\n",
    "    print(f\"Benchmarking {model_name}, {dataset} dataset\")\n",
    "    inference_df = benchmark_model(\n",
    "        model_name, dialect, input_file_path, output_file_path, db_root_dir\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
